export const metadata = {
  title: 'Configuration',
  description: 'Edgen configuration.',
}

# Configuration
The Edgen configuration. It is read from a file where you can define your models' locations, select which model to use for each endpoint, the number of threads Edgen can use and more. {{ className: 'lead' }}

| Config Name                       | Description                                | Default Value                                    |
| --------------------------------- | ------------------------------------------ | ------------------------------------------------ |
| `threads`                         | Number of CPU threads for processing       | \<number_physical_cores\> -1                     |
| `default_uri`                     | Default URI for communication              | http://127.0.0.1:33322                           |
| `chat_completions_models_dir`     | Directory for chat completions models      | `<DATA_DIR>/edgen/models/chat/completions`       |
| `chat_completions_model_name`     | Name of chat completions model             | neural-chat-7b-v3-3.Q4_K_M.gguf                  |
| `chat_completions_model_repo`     | HuggingFace repo for chat completions      | TheBloke/neural-chat-7B-v3-3-GGUF                |
| `audio_transcriptions_models_dir` | Directory for audio transcriptions models  | `<DATA_DIR>/edgen/models/audio/transcriptions`   |
| `audio_transcriptions_model_name` | Name of audio transcriptions model         | ggml-distil-small.en.bin                         |
| `audio_transcriptions_model_repo` | HuggingFace repo for audio transcriptions  | distil-whisper/distil-small.en                   |
| `gpu_policy`                      | Policy to choose how a model gets loaded   | !always_device                                   |

### Configuration Paths for DATA_DIR

| Platform | Value                                                             | Example                                           |
| -------- | ----------------------------------------------------------------- | ------------------------------------------------- |
| Linux    | `$XDG_DATA_HOME/_project_path_` or `$HOME/.local/share/_project_path_` | `/home/Alex/.local/share/edgen`                 |
| macOS    | `$HOME/Library/Application Support/_project_path_`               | `/Users/Alex/Library/Application Support/com.EdgenAI.Edgen`    |
| Windows  | `{FOLDERID_RoamingAppData}\_project_path_\data`                   | `C:\Users\Alex\AppData\Roaming\EdgenAI\Edgen\data` |

### GPU policies

Edgen supports the following policies, each with their own sub-settings:

    - `!always_device` - Models will always get loaded to a GPU.
        - `overflow_to_cpu` - If true, when a model can't be loaded to a GPU, it gets loaded to system memory. Else, Edgen will free GPU memory until the model can be loaded. **WARNING**: neither of these systems are currently implemented.
    - `!always_cpu` - Models will always get loaded to system memory.
        - `overflow_to_device` - If true, when a model can't be loaded to system memory, it gets loaded to a GPU. Else, Edgen will free system memory until the model can be loaded. **WARNING**: neither of these systems are currently implemented.
